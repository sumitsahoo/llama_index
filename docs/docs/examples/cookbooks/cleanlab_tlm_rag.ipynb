{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthy Retrieval-Augmented Generation with the Trustworthy Language Model\n",
    "\n",
    "This tutorial demonstrates how to use Cleanlab's Trustworthy Language Model (TLM) in any RAG system, to score the trustworthiness of answers and improve overall reliability of the RAG system.\n",
    "We recommend first completing the [TLM example tutorial](https://docs.llamaindex.ai/en/stable/examples/llm/cleanlab/).\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** has become popular for building LLM-based Question-Answer systems in domains where LLMs alone suffer from: hallucination, knowledge gaps, and factual inaccuracies. However, RAG systems often still produce unreliable responses, because they depend on LLMs that are fundamentally unreliable. Cleanlab's Trustworthy Language Model (TLM) offers a solution by providing trustworthiness scores to assess and improve response quality, **independent of your RAG architecture or retrieval and indexing processes**. \n",
    "\n",
    "To diagnose when RAG answers cannot be trusted, simply swap your existing LLM that is generating answers based on the retrieved context with TLM. This notebook showcases this for a standard RAG system, based off a tutorial in the popular [LlamaIndex](https://docs.llamaindex.ai/) framework. Here we merely replace the LLM used in the LlamaIndex tutorial with TLM, and showcase some of the benefits. TLM can be similarly inserted into *any* other RAG framework.\n",
    "\n",
    "![TLM RAG system correctly identifying high/low confidence responses](./thumbnail.png)\n",
    "\n",
    "## Setup\n",
    "\n",
    "RAG is all about connecting LLMs to data, to better inform their answers. This tutorial uses Nvidia's Q1 FY2024 earnings report as an example dataset.\n",
    "Use the following commands to download the data (earnings report) and store it in a directory named `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc 'https://cleanlab-public.s3.amazonaws.com/Datasets/NVIDIA_Financial_Results_Q1_FY2024.md'\n",
    "!mkdir -p ./data\n",
    "!mv NVIDIA_Financial_Results_Q1_FY2024.md data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index-llms-cleanlab llama-index llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize Cleanlab's TLM. Here we initialize a CleanlabTLM object with default settings. \n",
    "\n",
    "You can get your Cleanlab API key here: https://app.cleanlab.ai/account after creating an account. For detailed instructions, refer to [this guide](https://help.cleanlab.ai/guide/quickstart/api/#api-key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cleanlab import CleanlabTLM\n",
    "\n",
    "# set api key in env or in llm\n",
    "# import os\n",
    "# os.environ[\"CLEANLAB_API_KEY\"] = \"your api key\"\n",
    "\n",
    "llm = CleanlabTLM(api_key=\"your_api_key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you encounter `ValidationError` during the above import, please upgrade your python version to >= 3.11\n",
    "\n",
    "You can achieve better results by playing with the TLM configurations outlined in this [advanced TLM tutorial](https://help.cleanlab.ai/tutorials/tlm_advanced/).\n",
    "\n",
    "For example, if your application requires OpenAI's GPT-4 model and restrict the output tokens to 256, you can configure it using the `options` argument:\n",
    "\n",
    "```python\n",
    "options = {\n",
    "    \"model\": \"gpt-4\",\n",
    "    \"max_tokens\": 128,\n",
    "}\n",
    "llm = CleanlabTLM(api_key=\"your_api_key\", options=options)\n",
    "```\n",
    "\n",
    "Let's start by asking the LLM a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA's ticker symbol is NVDA.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"What is NVIDIA's ticker symbol?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM not only provides a response but also includes a **trustworthiness score** indicating the confidence that this response is good/accurate. You can access this score from the response itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trustworthiness_score': 0.9884869430083446}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.additional_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a RAG pipeline with TLM\n",
    "\n",
    "Now let's integrate TLM into a RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Embedding Model\n",
    "\n",
    "RAG uses an embedding model to match queries against document chunks to retrieve the most relevant data. Here we opt for a no-cost, local embedding model from Hugging Face. You can use any other embedding model by referring to this [LlamaIndex guide](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/#embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"BAAI/bge-small-en-v1.5\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Create Index + Query Engine\n",
    "\n",
    "Let's create an index from the documents stored in the data directory. The system can index multiple files within the same folder, although for this tutorial, we'll use just one document.\n",
    "We stick with the default index from LlamaIndex for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "# Optional step since we're loading just one data file\n",
    "for doc in documents:\n",
    "    doc.excluded_llm_metadata_keys.append(\n",
    "        \"file_path\"\n",
    "    )  # file_path wouldn't be a useful metadata to add to LLM's context since our datasource contains just 1 file\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated index is used to power a query engine over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TLM is agnostic to the index and the query engine used for RAG, and is compatible with any choices you make for these components of your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Trustworthiness Score from LLM response\n",
    "\n",
    "As we see above, Cleanlab's TLM also provides the `trustworthiness_score` in addition to the text, in its response to the prompt. \n",
    "\n",
    "To get this score out when TLM is used in a RAG pipeline, Llamaindex provides an [instrumentation](https://docs.llamaindex.ai/en/stable/module_guides/observability/instrumentation/#instrumentation) tool that allows us to observe the events running behind the scenes in RAG. <br> \n",
    "We can utilise this tooling to extract `trustworthiness_score` from LLM's response.\n",
    "\n",
    "Let's define a simple event handler that stores this score for every request sent to the LLM. You can refer to [Llamaindex's](https://docs.llamaindex.ai/en/stable/examples/instrumentation/basic_usage/) documentation for more details on instrumentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from llama_index.core.instrumentation.events import BaseEvent\n",
    "from llama_index.core.instrumentation.event_handlers import BaseEventHandler\n",
    "from llama_index.core.instrumentation import get_dispatcher\n",
    "from llama_index.core.instrumentation.events.llm import LLMCompletionEndEvent\n",
    "\n",
    "\n",
    "class GetTrustworthinessScore(BaseEventHandler):\n",
    "    events: List[BaseEvent] = []\n",
    "    trustworthiness_score = 0\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        \"\"\"Class name.\"\"\"\n",
    "        return \"GetTrustworthinessScore\"\n",
    "\n",
    "    def handle(self, event: BaseEvent) -> Dict:\n",
    "        if isinstance(event, LLMCompletionEndEvent):\n",
    "            self.trustworthiness_score = event.response.additional_kwargs[\n",
    "                \"trustworthiness_score\"\n",
    "            ]\n",
    "            self.events.append(event)\n",
    "\n",
    "\n",
    "# Root dispatcher\n",
    "root_dispatcher = get_dispatcher()\n",
    "\n",
    "# Register event handler\n",
    "event_handler = GetTrustworthinessScore()\n",
    "root_dispatcher.add_event_handler(event_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each query, we can fetch this score from `event_handler.trustworthiness_score`. Let's see it in action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering queries with our RAG system\n",
    "\n",
    "Let's try out our RAG pipeline based on TLM. Here we pose questions with differing levels of complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Define `display_response` helper function\n",
    "\n",
    "\n",
    "# This method presents formatted responses from our TLM-based RAG pipeline. It parses the output to display both the text response itself and the corresponding trustworthiness score.\n",
    "def display_response(response):\n",
    "    response_str = response.response\n",
    "    trustworthiness_score = event_handler.trustworthiness_score\n",
    "    print(f\"Response: {response_str}\")\n",
    "    print(f\"Trustworthiness score: {round(trustworthiness_score, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easy Questions\n",
    "\n",
    "We first pose straightforward questions that can be directly answered by the provided data and can be easily located within a few lines of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: NVIDIA's total revenue in the first quarter of fiscal 2024 was $7.19 billion.\n",
      "Trustworthiness score: 1.0\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was NVIDIA's total revenue in the first quarter of fiscal 2024?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The GAAP earnings per diluted share for the quarter (Q1 FY24) was $0.82.\n",
      "Trustworthiness score: 1.0\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What was the GAAP earnings per diluted share for the quarter?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Jensen Huang, NVIDIA's CEO, commented on the significant transitions the computer industry is undergoing, particularly in the areas of accelerated computing and generative AI.\n",
      "Trustworthiness score: 0.99\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What significant transitions did Jensen Huang, NVIDIA's CEO, comment on?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM returns high trustworthiness scores for these responses, indicating high confidence they are accurate. After doing a quick fact-check (reviewing the original earnings report), we can confirm that TLM indeed accurately answered these questions. In case you're curious, here are relevant excerpts from the data context for these questions:\n",
    "\n",
    "> NVIDIA (NASDAQ: NVDA) today reported revenue for the first quarter ended April 30, 2023, of $7.19 billion, ...\n",
    "\n",
    "> GAAP earnings per diluted share for the quarter were $0.82, up 28% from a year ago and up 44% from the previous quarter.\n",
    "\n",
    "> Jensen Huang, founder and CEO of NVIDIA, commented on the significant transitions the computer industry is undergoing, particularly accelerated computing and generative AI, ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions without Available Context \n",
    "\n",
    "Now let's see how TLM responds to queries that *cannot* be answered using the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report indicates that NVIDIA's professional visualization revenue declined by 53% year-over-year. While the specific factors contributing to this decline are not detailed in the provided information, several potential reasons can be inferred:\n",
      "\n",
      "1. **Market Conditions**: The overall market for professional visualization may have faced challenges, leading to reduced demand for NVIDIA's products in this segment.\n",
      "\n",
      "2. **Increased Competition**: The presence of competitors in the professional visualization space could have impacted NVIDIA's market share and revenue.\n",
      "\n",
      "3. **Economic Factors**: Broader economic conditions, such as inflation or reduced spending in industries that utilize professional visualization tools, may have contributed to the decline.\n",
      "\n",
      "4. **Transition to New Technologies**: The introduction of new technologies, such as the NVIDIA Omniverse™ Cloud, may have shifted focus away from traditional professional visualization products, affecting revenue.\n",
      "\n",
      "5. **Product Lifecycle**: If certain products were nearing the end of their lifecycle or if there were delays in new product launches, this could have impacted sales.\n",
      "\n",
      "Overall, while the report does not specify the exact reasons for the decline, these factors could be contributing elements based on industry trends and market dynamics.\n",
      "Trustworthiness score: 0.76\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"What factors as per the report were responsible to the decline in NVIDIA's proviz revenue?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower TLM trustworthiness score indicate a bit more uncertainty about the response, which aligns with the lack of information available. Let's try some more questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report indicates that NVIDIA's Gaming revenue decreased year over year by 38%, which is attributed to a combination of factors, including a challenging market environment and possibly reduced demand for gaming hardware. While specific reasons for the decline are not detailed in the provided information, the overall decrease in revenue suggests that the gaming sector is facing headwinds compared to the previous year.\n",
      "Trustworthiness score: 0.92\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does the report explain why NVIDIA's Gaming revenue decreased year over year?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The provided context information does not include any details about NVIDIA's dividend payout or the industry average for dividends. Therefore, I cannot provide a comparison of NVIDIA's dividend payout for this quarter to the industry average. Additional information regarding dividends would be needed to answer the query.\n",
      "Trustworthiness score: 0.87\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How does NVIDIA's dividend payout for this quarter compare to the industry average?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that TLM demonstrates the ability to recognize the limitations of the available information. It refrains from generating speculative responses or hallucinations, thereby maintaining the reliability of the question-answering system. This behavior showcases an understanding of the boundaries of the context and prioritizes accuracy over conjecture. \n",
    "\n",
    "### Challenging Questions\n",
    "\n",
    "Let's see how our RAG system responds to harder questions, some of which may be misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: NVIDIA's revenue for the first quarter of fiscal 2024 was $7.19 billion, and it was reported that this revenue was up 19% from the previous quarter. To find the revenue for the previous quarter, we can use the following calculation:\n",
      "\n",
      "Let \\( x \\) be the revenue for the previous quarter. \n",
      "\n",
      "The equation based on the 19% increase is:\n",
      "\\[ \n",
      "x + 0.19x = 7.19 \\text{ billion} \n",
      "\\]\n",
      "\\[ \n",
      "1.19x = 7.19 \\text{ billion} \n",
      "\\]\n",
      "\\[ \n",
      "x = \\frac{7.19 \\text{ billion}}{1.19} \\approx 6.04 \\text{ billion} \n",
      "\\]\n",
      "\n",
      "Now, to find the decrease in revenue from the previous quarter to this quarter:\n",
      "\\[ \n",
      "\\text{Decrease} = 7.19 \\text{ billion} - 6.04 \\text{ billion} \\approx 1.15 \\text{ billion} \n",
      "\\]\n",
      "\n",
      "Thus, NVIDIA's revenue decreased by approximately $1.15 billion this quarter compared to the last quarter.\n",
      "Trustworthiness score: 0.6\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How much did Nvidia's revenue decrease this quarter vs last quarter, in terms of $?\"\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The report mentions the following companies: Microsoft and Dell. ServiceNow is also mentioned in the context, but it is not specified in the provided highlights. Therefore, the companies explicitly mentioned in the report are Microsoft and Dell.\n",
      "Trustworthiness score: 0.6\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"This report focuses on Nvidia's Q1FY2024 financial results. There are mentions of other companies in the report like Microsoft, Dell, ServiceNow, etc. Can you name them all here?\",\n",
    ")\n",
    "display_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TLM automatically alerts us that these answers are unreliable, by the low trustworthiness score. RAG systems with TLM help you properly exercise caution when you see low trustworthiness scores. Here are the correct answers to the aforementioned questions:\n",
    "\n",
    "> NVIDIA's revenue increased by $1.14 billion this quarter compared to last quarter.\n",
    "\n",
    "> Google, Amazon Web Services, Microsoft, Oracle, ServiceNow, Medtronic, Dell Technologies\n",
    "\n",
    "With TLM, you can easily increase trust in any RAG system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
